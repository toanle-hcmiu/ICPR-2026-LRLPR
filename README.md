# Neuro-Symbolic LPR System

A sophisticated Automatic License Plate Recognition (ALPR) system for Brazilian license plates, combining deep neural networks with symbolic reasoning. Designed for the ICPR 2026 conference.

## Overview

This system recognizes Brazilian license plates in two formats:
- **Brazilian (Old)**: `LLL-NNNN` (e.g., ABC-1234)
- **Mercosul (New)**: `LLLNLNN` (e.g., ABC1D23)

### Key Features

- **Multi-frame Processing**: Processes 5 LR frames for robust recognition
- **Spatial Transformer Network**: Corrects geometric distortions with self-supervised alignment
- **Layout Classification**: Automatically detects Brazilian vs Mercosul format (with optional attention mechanism)
- **GAN Super-Resolution**: Full SwinIR-based image restoration (6 RSTB blocks, 180 embed dim)
- **Pretrained PARSeq**: Uses pretrained PARSeq from [baudm/parseq](https://github.com/baudm/parseq) for state-of-the-art OCR
- **Syntax-Masked Recognition**: Enforces valid plate formats using symbolic constraints
- **LCOFL Loss**: Layout and Character Oriented Focal Loss with SSIM and confusion matrix tracking
- **OCR-as-Discriminator**: Uses OCR confidence as GAN discriminator for stable training
- **Deformable Convolutions**: Adaptive spatial sampling for better character handling
- **Shared Attention Module**: PLTFAM-style attention with shared weights across blocks
- **Mixed Precision Training**: 2x faster training with automatic mixed precision (AMP)
- **EMA Model Averaging**: Exponential moving average for more stable final models
- **Stage-Aware Validation**: Prevents NaN losses during staged training
- **Soft Inference Mode**: Robust handling of damaged/non-standard plates

## Architecture

The system implements a **4-phase end-to-end pipeline** for low-resolution license plate recognition, combining deep learning with symbolic reasoning.

### High-Level Overview

```
Input: 5 LR Frames (16Ã—48Ã—3 each)
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: Feature Extraction & Geometric Alignment           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ SharedCNN   â”‚â”€â”€â–¶â”€â”€â”‚    STN      â”‚  Rectified Features    â”‚
â”‚  â”‚ Encoder     â”‚     â”‚ (Affine)    â”‚  (B,T,512,4,12)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: Classification & Frame Fusion                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Layout     â”‚     â”‚ Quality Scorer + Fusion â”‚            â”‚
â”‚  â”‚ Classifier  â”‚     â”‚ (Weighted Average)      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                        â”‚                          â”‚
â”‚    Layout Prob              Fused Feature                   â”‚
â”‚   (Brazilian/               (B,512,4,12)                    â”‚
â”‚    Mercosul)                     â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚  â”‚
                               â–¼  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: Super-Resolution (4Ã— Upscaling)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              SwinIR Generator                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚
â”‚  â”‚  â”‚ 6 RSTB Blocks (each with Shared Attention)      â”‚â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Swin Transformer Layers (window=8)            â”‚â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ 180 embed dim, 6 heads                        â”‚â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Deformable Conv support                       â”‚â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                     HR Image (64Ã—192Ã—3)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: Recognition & Neuro-Symbolic Decoding              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PARSeq      â”‚â”€â”€â–¶â”€â”€â”‚     Syntax Mask Layer           â”‚    â”‚
â”‚  â”‚ (Pretrained)â”‚     â”‚ (Dynamic Position Constraints)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                      â”‚                       â”‚
â”‚                              Masked Logits (B,7,39)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         Output: Plate Text (7 chars)
                         Example: "ABC1234" or "ABC1D23"
```

---

### Detailed Component Specifications

#### Phase 1: Feature Extraction & Geometric Alignment

| Component | Architecture | Input â†’ Output |
|-----------|--------------|----------------|
| **SharedCNNEncoder** | 4 Conv blocks (64â†’128â†’256â†’512 channels), each with Conv3Ã—3 + BN + ReLU + MaxPool2Ã—2 | `(B,T,3,16,48)` â†’ `(B,T,512,4,12)` |
| **SpatialTransformerNetwork** | Localization CNN + FC â†’ 6 affine params, then `grid_sample` | `(B,T,512,4,12)` â†’ Rectified `(B,T,512,4,12)` |
| **CornerPredictor** | GAP + FC(512â†’256â†’8) with tanh activation | `(B,T,512,4,12)` â†’ `(B,T,4,2)` corners |

**STN Transformation Constraints (tanh-bounded):**
| Parameter | Formula | Range | Purpose |
|-----------|---------|-------|---------|
| Scale | `1.0 + 0.5 Ã— tanh(x)` | [0.5, 1.5] | Uniform for x and y (rectangular output) |
| Shear | `0.1 Ã— tanh(x)` | [-0.1, 0.1] | Minimal shear to prevent parallelogram |
| Translation | `0.5 Ã— tanh(x)` | [-0.5, 0.5] | Bounded to keep plate in frame |

---

#### Phase 2: Classification & Frame Fusion

| Component | Architecture | Input â†’ Output |
|-----------|--------------|----------------|
| **LayoutClassifier** | GAP + FC(512â†’256â†’2) with optional attention | `(B,T,512,4,12)` â†’ `(B,2)` logits |
| **QualityScorerFusion** | Per-frame quality MLP + softmax â†’ weighted average | `(B,T,512,4,12)` â†’ `(B,512,4,12)` |

**Layout Classification:**
- Predicts Brazilian (class 0) vs Mercosul (class 1)
- Optional attention mechanism for difficult cases
- Output probability determines which syntax mask to apply

**Quality-Weighted Fusion:**
```
quality_score[t] = sigmoid(MLP(GAP(features[t])))
weights = softmax(quality_scores)
fused = Î£(weights[t] Ã— features[t])
```

---

#### Phase 3: Super-Resolution (SwinIR Generator)

**Architecture Configuration:**
```python
SwinIRGenerator(
    in_channels=512,        # From encoder (feature space SR)
    out_channels=3,         # RGB output
    embed_dim=180,          # Transformer embedding dimension
    depths=[6,6,6,6,6,6],   # 6 RSTB blocks
    num_heads=[6,6,6,6,6,6],# 6 attention heads per block
    window_size=8,          # Swin Transformer window size
    upscale=4,              # 4Ã— upscaling (16Ã—48 â†’ 64Ã—192)
    use_shared_attention=True,   # PLTFAM-style attention
    use_deformable=True          # Deformable conv in attention
)
```

**RSTB Block Structure:**
```
Input (B, L, C)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                  â”‚
    â–¼                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ Swin Transformer Layers    â”‚         â”‚
â”‚ (depth=6, window=8)        â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
              â”‚                        â”‚
              â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ Conv 3Ã—3 (residual path)   â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
              â”‚                        â”‚
              â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ Shared Attention Module    â”‚         â”‚
â”‚ (Channel + Positional +    â”‚         â”‚
â”‚  Geometrical Attention)    â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
              â”‚                        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (Residual Add)
                       â–¼
              Output (B, L, C)
```

**Shared Attention Module:**
| Component | Architecture | Purpose |
|-----------|--------------|---------|
| Channel Attention | GAP â†’ FC â†’ sigmoid | Channel-wise recalibration |
| Positional Attention | Conv â†’ sigmoid | Spatial position weighting |
| Geometrical Attention | 3Ã—3 Deformable Conv + sigmoid | Adaptive spatial sampling |

---

#### Phase 4: Recognition & Neuro-Symbolic Decoding

| Component | Architecture | Input â†’ Output |
|-----------|--------------|----------------|
| **PretrainedPARSeq** | ViT encoder (12 layers, 384 dim) + Transformer decoder | `(B,3,64,192)` â†’ `(B,7,39)` logits |
| **SyntaxMaskLayer** | Dynamic masking based on layout + position | Masks invalid characters per position |

**PARSeq OCR:**
- Uses pretrained weights from [baudm/parseq](https://github.com/baudm/parseq)
- Trained on scene text datasets (MJSynth, SynthText, etc.)
- Charset adapted from pretrained â†’ our 36-char vocabulary
- Falls back to custom implementation if pytorch_lightning unavailable

**Syntax Mask (Neuro-Symbolic Integration):**
```
Position:    0   1   2   3   4   5   6
Brazilian:  [L] [L] [L] [N] [N] [N] [N]   (L=letter, N=number)
Mercosul:   [L] [L] [L] [N] [L] [N] [N]
```

| Mode | Invalid Char Value | Use Case |
|------|-------------------|----------|
| Training | -100 | Soft masking for stable gradients |
| Hard Inference | -âˆ | Guarantees valid format output |
| Soft Inference | -50 | Allows exceptions for damaged plates |

---

### GAN Training Architecture

**Discriminators (Stage 2 & 3):**

| Discriminator | Purpose | Loss Type |
|---------------|---------|-----------|
| **PatchDiscriminator** | Binary real/fake classification | LSGAN (MSE) |
| **OCR Discriminator** | Recognition-based quality | Confidence + CE |

**PatchDiscriminator Architecture:**
```python
PatchDiscriminator(
    in_channels=3,
    base_channels=64,
    num_layers=3,     # 64 â†’ 128 â†’ 256 â†’ 512
    use_spectral_norm=True
)
# Output: (B, 1, H/8, W/8) patch scores
```

**OCR Discriminator:**
- Uses the model's own PARSeq recognizer
- Measures recognition confidence on generated images
- Skipped when `ocr_real_conf < 10%` (untrained recognizer)

---

### Training Stages

| Stage | Name | Modules Trained | Modules Frozen | Epochs |
|-------|------|-----------------|----------------|--------|
| 1 | STN | Encoder, STN, CornerPredictor | Generator, Recognizer | 50 |
| 2 | Restoration | Generator, LayoutClassifier, Fusion | Encoder, STN, Recognizer | 100 |
| 3 | Full | All modules | None | 50 |

**Loss Functions per Stage:**

| Stage | Losses | Formula |
|-------|--------|---------|
| **STN** | Self-supervised + Pixel | `L_identity + L_consistency + L_smoothness + L_pixel` |
| **Restoration** | Pixel + GAN + Layout + OCR Guidance | `L_pixel + 0.1Ã—L_GAN + 0.1Ã—L_layout + L_ocr_guidance` |
| **Full** | All | `L_pixel + 0.1Ã—L_GAN + 0.5Ã—L_OCR + 0.1Ã—L_geo + 0.1Ã—L_layout` |

---

### Data Flow Summary

```
Input:  (B, 5, 3, 16, 48)     # 5 LR frames, 16Ã—48 RGB each
          â†“
Encoder:  (B, 5, 512, 4, 12)  # Feature maps
          â†“
STN:      (B, 5, 512, 4, 12)  # Rectified features
          â†“
Layout:   (B, 2)              # Brazilian/Mercosul logits
          â†“
Fusion:   (B, 512, 4, 12)     # Quality-weighted single feature
          â†“
SwinIR:   (B, 3, 64, 192)     # Super-resolved HR image (4Ã— upscale)
          â†“
PARSeq:   (B, 7, 39)          # Raw logits (7 positions Ã— 39 vocab)
          â†“
Mask:     (B, 7, 39)          # Valid chars only (position-constrained)
          â†“
Output:   ["ABC1234", ...]    # Decoded plate strings
```



## Installation

```bash
# Clone the repository
git clone https://github.com/your-repo/ICPR-2026-LRLPR.git
cd ICPR-2026-LRLPR

# Create virtual environment (Python 3.10+ recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- ~8GB VRAM for training with batch size 32

## Usage

### Training

The system uses a staged training schedule:

```bash
# Train all stages (with defaults: AMP enabled, EMA enabled)
python train.py --data-dir data/ --output-dir outputs/

# Train specific stage
python train.py --stage 1 --data-dir data/  # STN only
python train.py --stage 3 --resume checkpoints/step2.pth  # Fine-tune

# Training stages:
# 0: Synthetic pre-training (PARSeq) - NOT USED when using pretrained
# 1: Geometry warm-up (STN) - Self-supervised + pixel loss
# 2: Restoration + Layout (SwinIR, Classifier)
# 3: End-to-end fine-tuning (All modules)
```

#### Advanced Training Options

```bash
# Full training with all options
python train.py --data-dir data/ --output-dir outputs/ \
    --stage all \
    --early-stopping 15      # Stop if no improvement for 15 epochs

# Disable mixed precision (for debugging or CPU training)
python train.py --no-amp --data-dir data/

# Disable EMA model averaging
python train.py --no-ema --data-dir data/

# Resume from checkpoint
python train.py --stage 3 --resume checkpoints/restoration_best.pth
```

#### Training Features

| Feature | Default | Description |
|---------|---------|-------------|
| **Mixed Precision (AMP)** | Enabled | 2x faster training, lower memory usage |
| **EMA** | Enabled | Exponential moving average of weights for stable models |
| **Early Stopping** | Disabled | Stop training if validation accuracy plateaus |
| **LR Warmup** | 5 epochs | Linear warmup before main scheduler |
| **Gradient Clipping** | 1.0 (0.5 for STN) | Prevents gradient explosion |
| **Stage-Aware Validation** | Enabled | Uses stage-specific loss to prevent NaN |

#### Training Stages

| Stage | Modules Trained | Loss Functions | Batch Size |
|-------|-----------------|----------------|------------|
| STN | Encoder, STN | Self-supervised, Pixel, Corner | 32 |
| Restoration | Generator, Layout, Fusion | Pixel, GAN, Layout | 32 |
| Full | All | Pixel, GAN, OCR, Geometry, Layout | 8 |

#### Metrics Tracked

- **Plate Accuracy**: Exact match (all 7 characters correct)
- **Character Accuracy**: Per-character accuracy (excluding special tokens)
- **Layout Accuracy**: Brazilian vs Mercosul classification accuracy

### Inference

```bash
# Single image
python inference.py --model checkpoints/best.pth --input image.jpg

# Batch processing
python inference.py --model checkpoints/best.pth --input folder/

# Video processing
python inference.py --model checkpoints/best.pth --input video.mp4
```

### Python API

```python
from models import NeuroSymbolicLPR
from inference import load_model, predict_single
import torch
from PIL import Image
import numpy as np

# Load model with default settings
device = torch.device('cuda')
model = load_model('checkpoints/best.pth', device)

# Run inference
image = np.array(Image.open('plate.jpg').convert('RGB'))
text, confidence, sr_image, is_mercosul = predict_single(model, image, device)

print(f"Plate: {text}")
print(f"Format: {'Mercosul' if is_mercosul else 'Brazilian'}")
print(f"Confidence: {confidence:.2%}")
```

### Model Configuration Options

```python
from models import NeuroSymbolicLPR

# Create model with all options
model = NeuroSymbolicLPR(
    num_frames=5,
    lr_size=(16, 48),
    hr_size=(64, 192),
    
    # Use attention-enhanced layout classifier (recommended for difficult cases)
    use_attention_layout=True,
    
    # Soft inference for robustness to damaged/non-standard plates
    soft_inference=True,
    soft_inference_value=-50.0,
    
    # Use pretrained PARSeq (recommended)
    use_pretrained_parseq=True,
    parseq_model_name='parseq',  # or 'parseq_tiny' for faster inference
    
    # SwinIR configuration (full model, not lightweight)
    swinir_embed_dim=180,  # Full SwinIR uses 180
    swinir_depths=[6, 6, 6, 6, 6, 6],  # 6 RSTB blocks
    swinir_num_heads=[6, 6, 6, 6, 6, 6],
    swinir_window_size=8,
)
```

## Project Structure

```
ICPR-2026-LRLPR/
â”œâ”€â”€ config.py                 # Configuration and hyperparameters
â”œâ”€â”€ train.py                  # Training script with staged training
â”œâ”€â”€ inference.py              # Inference script
â”œâ”€â”€ requirements.txt          # Dependencies
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ neuro_symbolic_lpr.py # Main model (4-phase pipeline)
â”‚   â”œâ”€â”€ encoder.py            # Shared CNN encoder (4 blocks)
â”‚   â”œâ”€â”€ stn.py                # Spatial Transformer Network + MultiFrameSTN
â”‚   â”œâ”€â”€ layout_classifier.py  # Layout classifier (+ attention variant)
â”‚   â”œâ”€â”€ feature_fusion.py     # Quality scorer & weighted fusion
â”‚   â”œâ”€â”€ swinir.py             # Full SwinIR generator
â”‚   â”œâ”€â”€ discriminator.py      # PatchGAN discriminator
â”‚   â”œâ”€â”€ parseq.py             # PARSeq wrapper (pretrained + custom fallback)
â”‚   â”œâ”€â”€ syntax_mask.py        # Dynamic syntax mask with soft inference
â”‚   â”œâ”€â”€ deformable_conv.py    # Deformable Convolution v2
â”‚   â””â”€â”€ shared_attention.py   # PLTFAM-style shared attention module
â”‚
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ corner_loss.py        # STN corner supervision
â”‚   â”œâ”€â”€ gan_loss.py           # Adversarial losses (vanilla, lsgan, wgan)
â”‚   â”œâ”€â”€ composite_loss.py     # Combined loss + SelfSupervisedSTNLoss
â”‚   â”œâ”€â”€ ocr_perceptual_loss.py # OCR-aware perceptual losses
â”‚   â”œâ”€â”€ lcofl_loss.py         # LCOFL loss with SSIM and confusion tracking
â”‚   â””â”€â”€ ocr_discriminator.py  # OCR-as-Discriminator for stable GAN training
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py            # RodoSolDataset + SyntheticLPRDataset
â”‚   â”œâ”€â”€ augmentation.py       # Style-aware augmentation pipeline
â”‚   â”œâ”€â”€ train/                # Training data
â”‚   â”œâ”€â”€ val/                  # Validation data
â”‚   â””â”€â”€ test/                 # Test data
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ split_dataset.py      # Dataset splitting utility
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ visualization.py      # Visualization tools
```

## Loss Functions

### Composite Training Loss

The total loss for end-to-end training:

```
L_total = L_pixel + 0.1 Ã— L_GAN + 0.5 Ã— L_OCR + 0.1 Ã— L_geo + 0.1 Ã— L_layout
```

| Loss | Weight | Description |
|------|--------|-------------|
| L_pixel | 1.0 | L1 pixel reconstruction loss |
| L_GAN | 0.1 | Adversarial loss for realism |
| L_OCR | 0.5 | Cross-entropy character recognition loss |
| L_geo | 0.1 | Corner loss for STN (+ self-supervised) |
| L_layout | 0.1 | Binary cross-entropy for layout classification |

### Self-Supervised STN Loss

For training without corner annotations (used in STN stage):

```python
L_stn = w_id Ã— L_identity + w_cons Ã— L_consistency + w_smooth Ã— L_smoothness
```

| Component | Weight | Description |
|-----------|--------|-------------|
| Identity | 0.1 | Prevents collapse, keeps transforms near identity |
| Consistency | 1.0 | Encourages similar transforms across frames |
| Smoothness | 0.5 | Penalizes extreme scaling/rotation/shear |

### OCR-Aware Perceptual Losses

Additional losses in `losses/ocr_perceptual_loss.py`:

| Loss Class | Description |
|------------|-------------|
| `OCRAwarePerceptualLoss` | Uses downstream OCR model to guide restoration |
| `CharacterFocusLoss` | Edge-aware loss using Sobel operators |
| `MultiScaleOCRLoss` | Evaluates OCR at multiple scales (1.0Ã—, 0.5Ã—, 0.25Ã—) |

### LCOFL Loss (Layout and Character Oriented Focal Loss)

From "Enhancing License Plate Super-Resolution" (Nascimento et al.):

```python
# Enable in config
config.training.use_lcofl = True
config.training.weight_lcofl = 0.5
```

| Component | Description |
|-----------|-------------|
| Classification Loss | Weighted cross-entropy with dynamic character weights |
| Layout Penalty | Penalizes digit/letter misplacements based on format |
| SSIM Loss | Structural similarity for image quality |
| Confusion Tracking | Increases weights for frequently confused character pairs |

## Plate Format Specifications (Hardcoded)

### Brazilian Format (Old)
- **Pattern**: `LLL-NNNN` (displayed with dash) or `LLLNNNN` (stored without dash)
- **Regex**: `^[A-Z]{3}[0-9]{4}$`
- **Position Constraints**: `[L, L, L, N, N, N, N]`
- **Example**: `ABC1234` â†’ `ABC-1234`

### Mercosul Format (New)
- **Pattern**: `LLLNLNN` (no dash)
- **Regex**: `^[A-Z]{3}[0-9][A-Z][0-9]{2}$`
- **Position Constraints**: `[L, L, L, N, L, N, N]`
- **Example**: `ABC1D23`

### Hardcoded Constants

```python
PLATE_LENGTH = 7
VOCAB_SIZE = 39  # 36 chars (A-Z, 0-9) + 3 special tokens (PAD, BOS, EOS)
CHARSET = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
```

**Important**: These formats are hardcoded in:
- `config.py`: Pattern definitions and position constraints
- `models/syntax_mask.py`: Mask generation logic
- `data/dataset.py`: Text validation and layout inference

## Syntax Mask

The key neuro-symbolic innovation. The mask enforces valid formats dynamically based on layout prediction:

### Masking Modes

| Mode | Mask Value | Use Case |
|------|------------|----------|
| **Training** | `-100` | Soft mask for gradient stability |
| **Hard Inference** | `-inf` | Guarantees valid output (default) |
| **Soft Inference** | `-50` | Allows invalid chars if evidence is strong |

## Dataset

Designed for the RodoSol-ALPR dataset format:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ img_0001.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ annotations.json
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â””â”€â”€ ...
```

Annotations format:
```json
{
  "img_0001.jpg": {
    "text": "ABC1234",
    "corners": [[x1,y1], [x2,y2], [x3,y3], [x4,y4]],
    "layout": "brazilian"
  }
}
```

## Reproducibility & Determinism

By default, this codebase enables **strict determinism** for exact reproducibility across training runs. This is controlled by `seed_everything()` in `train.py`.

### What's Enabled by Default

| Setting | Value | Purpose |
|---------|-------|---------|
| `torch.backends.cudnn.deterministic` | `True` | Force deterministic cuDNN algorithms |
| `torch.backends.cudnn.benchmark` | `False` | Disable cuDNN autotuning |
| `torch.use_deterministic_algorithms` | `True` | Error on non-deterministic ops |
| `CUBLAS_WORKSPACE_CONFIG` | `:4096:8` | Deterministic cuBLAS |
| TF32 | Disabled | Exact float32 precision |
| `PYTHONHASHSEED` | Set to seed | Deterministic Python hashing |

### Performance Impact

Strict determinism typically has a **5-15% performance overhead** compared to non-deterministic training. This is the tradeoff for exact reproducibility.

To disable strict determinism for faster training (at the cost of reproducibility):

```python
# In your training script
seed_everything(42, strict_determinism=False)
```

### Deterministic Inference

Inference is deterministic by default. The `--frame-noise-std` flag (default: `0.0`) controls whether extra frames have random noise added:

```bash
# Deterministic inference (default)
python inference.py --model checkpoint.pth --input image.jpg

# With frame augmentation (non-deterministic)
python inference.py --model checkpoint.pth --input image.jpg --frame-noise-std 0.01
```

## Checkpoint Security

**Warning:** This codebase uses `torch.load()` for checkpoint loading, which internally uses Python's `pickle` module. Pickle can execute arbitrary code during deserialization.

### Security Guidelines

1. **Only load checkpoints from trusted sources** - Never load `.pth` files from untrusted origins
2. **Verify checkpoint integrity** - Use checksums (SHA-256) when downloading checkpoints
3. **Local files only** - The loading functions reject non-file paths (e.g., URLs)

When loading a checkpoint, you'll see a security warning:

```
UserWarning: Loading checkpoint from 'path/to/model.pth'. 
torch.load() uses pickle which can execute arbitrary code. 
Only load checkpoints from trusted sources.
```

### Future Improvements

For enhanced security, consider migrating to [safetensors](https://github.com/huggingface/safetensors) format, which is a safe-by-design serialization format that doesn't support code execution.

## Known Issues & Solutions

### OCR Loss NaN During STN Stage
**Issue**: Validation shows `ocr: nan` during STN training stage.  
**Cause**: Pretrained PARSeq produces logits with unmapped vocabulary positions initialized to -100, causing CrossEntropyLoss to produce NaN.  
**Solution**: Stage-aware validation now uses stage-specific losses, avoiding OCR loss computation during STN stage.

### Gradient Explosion in STN
**Issue**: NaN loss values after several epochs of STN training.  
**Cause**: STN transformation parameters can explode during training.  
**Solution**: 
- Reduced STN learning rate to 5e-5 (from 1e-4)
- Tighter gradient clipping for STN stage (0.5 vs 1.0)
- Self-supervised STN loss with clamping and numerical safeguards

## Changelog

### v1.5.0 (Latest)

**Stage 3 Anti-Collapse Training:**
- âœ… Frozen OCR for LCOFL classification - prevents mode collapse by using a frozen copy of PARSeq for loss computation
- âœ… SR Anchor Loss - anchors Stage 3 output to Stage 2 for visual quality preservation
- âœ… GAN disabled in Stage 3 - using OCR-only discriminator approach from original LCOFL paper
- âœ… LCOFL classification active from start (no curriculum) - matches original paper configuration

**Configuration Updates:**
- âœ… New config parameter: `use_frozen_ocr_for_lcofl` (default: True)
- âœ… LCOFL weight increased to 0.75 (matching original paper)
- âœ… Edge loss weight set to 0.5 for sharper character boundaries
- âœ… Stage 3 OCR parameters: warmup 6000 steps, ramp 6000 steps, max weight 0.5

**Documentation:**
- âœ… Updated AGENTS.md with Stage 3 training patterns
- âœ… Added frozen OCR integration guidance

### v1.4.0

**Training Improvements:**
- âœ… Stage-aware validation to prevent NaN losses
- âœ… Self-supervised STN loss with numerical stability safeguards
- âœ… Improved gradient clipping per stage
- âœ… Better handling of invalid layout labels (-1)

**Model Updates:**
- âœ… Full SwinIR architecture (6 RSTB blocks, 180 embed dim)
- âœ… Pretrained PARSeq integration with charset adaptation
- âœ… Fallback to custom PARSeq if torch.hub fails

**Bug Fixes:**
- ğŸ› Fixed OCR loss NaN during STN stage validation
- ğŸ› Fixed loss accumulation skipping NaN values

### v1.4.0 (Current)

**GAN Training Stability Fixes:**
- ğŸ› Fixed LSGAN mismatch: Generator used MSE but Discriminator used BCE
- ğŸ› Fixed warm-up period: GAN loss now completely disabled during warm-up
- ğŸ› Fixed GAN weight cap: Was capped at 0.01 instead of config value 0.05
- ğŸ› Fixed R1 penalty: Reduced from 5.0 to 1.0 for LSGAN stability  
- ğŸ› Fixed validation loss: Was returning 0.0 due to NaN check on all outputs
- âœ… Added `--reset-epoch` flag for resuming training with epoch reset
- âœ… Increased warm-up epochs from 5 to 10

**Documentation:**
- âœ… Updated AGENTS.md with known issues and unintegrated features
- âœ… Clarified features implemented but not yet integrated:
  - OCR-as-Discriminator (`losses/ocr_discriminator.py`) - NOT integrated
  - Deformable Conv (`models/deformable_conv.py`) - NOT integrated  
  - Shared Attention (`models/shared_attention.py`) - NOT integrated

### v1.3.0

**LCOFL Paper Implementation:**
- âœ… LCOFL Loss with 4 components (classification, layout penalty, SSIM, confusion tracking)
- âœ… Deformable Convolution v2 module (implemented, not integrated)
- âœ… PLTFAM-style Shared Attention Module (implemented, not integrated)
- âœ… OCR-as-Discriminator module (implemented, not integrated)
- âœ… Confusion matrix tracking during validation
- âœ… Dynamic character weight updates based on confusion pairs

### v1.1.0

**Training Improvements:**
- âœ… Mixed precision training (AMP) for 2x faster training
- âœ… EMA (Exponential Moving Average) for stable model weights
- âœ… Early stopping with configurable patience
- âœ… Learning rate warmup

**Model Enhancements:**
- âœ… Attention-enhanced layout classifier
- âœ… Soft inference constraints for damaged plates

**New Loss Functions:**
- âœ… `OCRAwarePerceptualLoss`
- âœ… `CharacterFocusLoss`
- âœ… `MultiScaleOCRLoss`

### v1.1.1 (2026-01-19)

**Bug Fixes:**
- Fixed PARSeq pretrained model staying in training mode â†’ random outputs
- Fixed PARSeq charset adapter overwriting lowercase with uppercase logits
- Fixed VGG perceptual loss feature extraction feeding wrong shapes
- Fixed perceptual loss not being computed in restoration stage
- Enabled perceptual loss in Stage 2 training (weight=0.1)

**Improvements:**
- Added TensorBoard visualizations for STN, LR images, and OCR predictions
- Added OCR confidence logging per sample

### v1.0.0

- Initial implementation of the Neuro-Symbolic LPR system
- 4-phase pipeline: STN â†’ Layout/Fusion â†’ SwinIR â†’ PARSeq
- Syntax-masked recognition for valid plate outputs
- Staged training schedule

## Citation

If you use this code, please cite:

```bibtex
@inproceedings{neurosymboliclpr2026,
  title={Neuro-Symbolic License Plate Recognition for Brazilian Plates},
  author={Your Name},
  booktitle={ICPR 2026},
  year={2026}
}
```

## License

MIT License
